{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data500K_10c.csv\", header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[:25]\n",
    "df.to_csv(\"test.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "import os\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "import datetime\n",
    "import numpy as np\n",
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn.svm import SVC     ### SVM for classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "import math\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0,[1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0])\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "conf.setMaster('local')\n",
    "conf.setAppName('spark-basic')\n",
    "# sc = SparkContext(conf=conf)\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "\n",
    "df= sc.textFile(\"test.csv\").map(lambda line: line.split(\",\"))\n",
    "                                        #label , features\n",
    "dataset  = df.map(lambda x: LabeledPoint(x[0], x[1:]))\n",
    "\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3])\n",
    "\n",
    "\n",
    "print (dataset.collect()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 7 8 0 4 2 6 5 3 1]\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"test.csv\")\n",
    "a = test.iloc[:,[0]]\n",
    "b = pd.unique(a.values.ravel())\n",
    "print (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/17 12:19:03 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 16 (= number of training instances)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree -----------------\n",
      "\n",
      "Runtime :  0:00:00.605511\n",
      "Test Accurancy = 0.2\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "model = DecisionTree.trainClassifier(trainingData,  numClasses=20, categoricalFeaturesInfo={}, impurity='gini', maxDepth=8, maxBins=32)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))    \n",
    "# print(predictions.collect())\n",
    "\n",
    "end = datetime.now() -start\n",
    "\n",
    "print (\"Decision Tree -----------------\\n\")\n",
    "print ('Runtime : ', end)\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "\n",
    "#    print(labelsAndPredictions.collect()[1])\n",
    "\n",
    "acc =0\n",
    "acc = (labelsAndPredictions.filter( lambda lp: lp[0] == lp[1]).count()) / float(testData.count())\n",
    "print('Test Accurancy = ' + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes -----------------\n",
      "\n",
      "Runtime :  0:00:00.662869\n",
      "Test Accuracy 0.6\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "model = NaiveBayes.train(trainingData, 1.0)\n",
    "end = datetime.now() -start\n",
    "print (\"\\nNaive Bayes -----------------\\n\")\n",
    "print ('Runtime : ', end)\n",
    "# Make prediction and test accuracy.\n",
    "predictionAndLabel = testData.map(lambda p: (model.predict(p.features), p.label))\n",
    "accuracy = 1.0 * predictionAndLabel.filter(lambda pl: pl[0] == pl[1]).count() / testData.count()\n",
    "print('Test Accuracy {}'.format(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/17 12:18:55 ERROR DataValidators: Classification labels should be 0 or 1. Found 17 invalid labels\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o314.trainSVMModelWithSGD.\n: org.apache.spark.SparkException: Input validation failed.\n\tat org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm.run(GeneralizedLinearAlgorithm.scala:252)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainRegressionModel(PythonMLLibAPI.scala:92)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainSVMModelWithSGD(PythonMLLibAPI.scala:251)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/totoo/Code/university/Big_data/Spark/preprocess.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/totoo/Code/university/Big_data/Spark/preprocess.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m start \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/totoo/Code/university/Big_data/Spark/preprocess.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m SVMWithSGD\u001b[39m.\u001b[39;49mtrain(trainingData, iterations\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/totoo/Code/university/Big_data/Spark/preprocess.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m end \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow() \u001b[39m-\u001b[39mstart\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/totoo/Code/university/Big_data/Spark/preprocess.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mSVM -----------------\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/mllib/classification.py:729\u001b[0m, in \u001b[0;36mSVMWithSGD.train\u001b[0;34m(cls, data, iterations, step, regParam, miniBatchFraction, initialWeights, regType, intercept, validateData, convergenceTol)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(rdd: RDD[LabeledPoint], i: Vector) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterable[Any]:\n\u001b[1;32m    715\u001b[0m     \u001b[39mreturn\u001b[39;00m callMLlibFunc(\n\u001b[1;32m    716\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtrainSVMModelWithSGD\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    717\u001b[0m         rdd,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[39mfloat\u001b[39m(convergenceTol),\n\u001b[1;32m    727\u001b[0m     )\n\u001b[0;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m _regression_train_wrapper(train, SVMModel, data, initialWeights)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/mllib/regression.py:284\u001b[0m, in \u001b[0;36m_regression_train_wrapper\u001b[0;34m(train_func, modelClass, data, initial_weights)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m modelClass(weights, intercept, numFeatures, numClasses)  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     weights, intercept \u001b[39m=\u001b[39m train_func(data, _convert_to_vector(initial_weights))\n\u001b[1;32m    285\u001b[0m     \u001b[39mreturn\u001b[39;00m modelClass(weights, intercept)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/mllib/classification.py:715\u001b[0m, in \u001b[0;36mSVMWithSGD.train.<locals>.train\u001b[0;34m(rdd, i)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(rdd: RDD[LabeledPoint], i: Vector) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterable[Any]:\n\u001b[0;32m--> 715\u001b[0m     \u001b[39mreturn\u001b[39;00m callMLlibFunc(\n\u001b[1;32m    716\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtrainSVMModelWithSGD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    717\u001b[0m         rdd,\n\u001b[1;32m    718\u001b[0m         \u001b[39mint\u001b[39;49m(iterations),\n\u001b[1;32m    719\u001b[0m         \u001b[39mfloat\u001b[39;49m(step),\n\u001b[1;32m    720\u001b[0m         \u001b[39mfloat\u001b[39;49m(regParam),\n\u001b[1;32m    721\u001b[0m         \u001b[39mfloat\u001b[39;49m(miniBatchFraction),\n\u001b[1;32m    722\u001b[0m         i,\n\u001b[1;32m    723\u001b[0m         regType,\n\u001b[1;32m    724\u001b[0m         \u001b[39mbool\u001b[39;49m(intercept),\n\u001b[1;32m    725\u001b[0m         \u001b[39mbool\u001b[39;49m(validateData),\n\u001b[1;32m    726\u001b[0m         \u001b[39mfloat\u001b[39;49m(convergenceTol),\n\u001b[1;32m    727\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/mllib/common.py:139\u001b[0m, in \u001b[0;36mcallMLlibFunc\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39massert\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    138\u001b[0m api \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(sc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonMLLibAPI(), name)\n\u001b[0;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m callJavaFunc(sc, api, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/mllib/common.py:131\u001b[0m, in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m\"\"\"Call Java Function\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m java_args \u001b[39m=\u001b[39m [_py2java(sc, a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args]\n\u001b[0;32m--> 131\u001b[0m \u001b[39mreturn\u001b[39;00m _java2py(sc, func(\u001b[39m*\u001b[39;49mjava_args))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o314.trainSVMModelWithSGD.\n: org.apache.spark.SparkException: Input validation failed.\n\tat org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm.run(GeneralizedLinearAlgorithm.scala:252)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainRegressionModel(PythonMLLibAPI.scala:92)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainSVMModelWithSGD(PythonMLLibAPI.scala:251)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "model = SVMWithSGD.train(trainingData, iterations=10)\n",
    "end = datetime.now() -start\n",
    "\n",
    "print (\"\\nSVM -----------------\\n\")\n",
    "print ('Runtime : ', end)\n",
    "# Evaluating the model on training data\n",
    "labelsAndPreds = testData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "acc = labelsAndPreds.filter(lambda lp: lp[0] == lp[1]).count() / float(testData.count())\n",
    "print(\"Test Accurancy = \" + str(acc))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/17 12:26:36 WARN Instrumentation: [13c30597] Initial coefficients will be ignored! Its dimensions (1, 100) did not match the expected size (10, 100)\n",
      "23/04/17 12:26:36 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/04/17 12:26:36 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o459.trainLogisticRegressionModelWithLBFGS.\n: org.apache.spark.SparkException: Multinomial models contain a matrix of coefficients, use coefficientMatrix instead.\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.coefficients(LogisticRegression.scala:1085)\n\tat org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS.runWithMlLogisticRegression$1(LogisticRegression.scala:344)\n\tat org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS.run(LogisticRegression.scala:348)\n\tat org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS.run(LogisticRegression.scala:316)\n\tat org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS.run(LogisticRegression.scala:246)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainRegressionModel(PythonMLLibAPI.scala:92)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainLogisticRegressionModelWithLBFGS(PythonMLLibAPI.scala:311)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/totoo/Code/university/Big_data/Spark/preprocess.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/totoo/Code/university/Big_data/Spark/preprocess.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m start \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/totoo/Code/university/Big_data/Spark/preprocess.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m LogisticRegressionWithLBFGS\u001b[39m.\u001b[39;49mtrain(trainingData)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/totoo/Code/university/Big_data/Spark/preprocess.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m end \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow() \u001b[39m-\u001b[39mstart\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/totoo/Code/university/Big_data/Spark/preprocess.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mLogisticRegression -----------------\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/mllib/classification.py:526\u001b[0m, in \u001b[0;36mLogisticRegressionWithLBFGS.train\u001b[0;34m(cls, data, iterations, initialWeights, regParam, regType, intercept, corrections, tolerance, validateData, numClasses)\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    525\u001b[0m             initialWeights \u001b[39m=\u001b[39m [\u001b[39m0.0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39mfirst()\u001b[39m.\u001b[39mfeatures) \u001b[39m*\u001b[39m (numClasses \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 526\u001b[0m \u001b[39mreturn\u001b[39;00m _regression_train_wrapper(train, LogisticRegressionModel, data, initialWeights)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/mllib/regression.py:279\u001b[0m, in \u001b[0;36m_regression_train_wrapper\u001b[0;34m(train_func, modelClass, data, initial_weights)\u001b[0m\n\u001b[1;32m    277\u001b[0m     initial_weights \u001b[39m=\u001b[39m [\u001b[39m0.0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39mfirst()\u001b[39m.\u001b[39mfeatures)\n\u001b[1;32m    278\u001b[0m \u001b[39mif\u001b[39;00m modelClass \u001b[39m==\u001b[39m LogisticRegressionModel:\n\u001b[0;32m--> 279\u001b[0m     weights, intercept, numFeatures, numClasses \u001b[39m=\u001b[39m train_func(\n\u001b[1;32m    280\u001b[0m         data, _convert_to_vector(initial_weights)\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m modelClass(weights, intercept, numFeatures, numClasses)  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/mllib/classification.py:504\u001b[0m, in \u001b[0;36mLogisticRegressionWithLBFGS.train.<locals>.train\u001b[0;34m(rdd, i)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(rdd: RDD[LabeledPoint], i: Vector) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterable[Any]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[39mreturn\u001b[39;00m callMLlibFunc(\n\u001b[1;32m    505\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtrainLogisticRegressionModelWithLBFGS\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    506\u001b[0m         rdd,\n\u001b[1;32m    507\u001b[0m         \u001b[39mint\u001b[39;49m(iterations),\n\u001b[1;32m    508\u001b[0m         i,\n\u001b[1;32m    509\u001b[0m         \u001b[39mfloat\u001b[39;49m(regParam),\n\u001b[1;32m    510\u001b[0m         regType,\n\u001b[1;32m    511\u001b[0m         \u001b[39mbool\u001b[39;49m(intercept),\n\u001b[1;32m    512\u001b[0m         \u001b[39mint\u001b[39;49m(corrections),\n\u001b[1;32m    513\u001b[0m         \u001b[39mfloat\u001b[39;49m(tolerance),\n\u001b[1;32m    514\u001b[0m         \u001b[39mbool\u001b[39;49m(validateData),\n\u001b[1;32m    515\u001b[0m         \u001b[39mint\u001b[39;49m(numClasses),\n\u001b[1;32m    516\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/mllib/common.py:139\u001b[0m, in \u001b[0;36mcallMLlibFunc\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39massert\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    138\u001b[0m api \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(sc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonMLLibAPI(), name)\n\u001b[0;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m callJavaFunc(sc, api, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/mllib/common.py:131\u001b[0m, in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m\"\"\"Call Java Function\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m java_args \u001b[39m=\u001b[39m [_py2java(sc, a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args]\n\u001b[0;32m--> 131\u001b[0m \u001b[39mreturn\u001b[39;00m _java2py(sc, func(\u001b[39m*\u001b[39;49mjava_args))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o459.trainLogisticRegressionModelWithLBFGS.\n: org.apache.spark.SparkException: Multinomial models contain a matrix of coefficients, use coefficientMatrix instead.\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.coefficients(LogisticRegression.scala:1085)\n\tat org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS.runWithMlLogisticRegression$1(LogisticRegression.scala:344)\n\tat org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS.run(LogisticRegression.scala:348)\n\tat org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS.run(LogisticRegression.scala:316)\n\tat org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS.run(LogisticRegression.scala:246)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainRegressionModel(PythonMLLibAPI.scala:92)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainLogisticRegressionModelWithLBFGS(PythonMLLibAPI.scala:311)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "model = LogisticRegressionWithLBFGS.train(trainingData, iterations=10,numClasses=)\n",
    "end = datetime.now() -start\n",
    "print (\"\\nLogisticRegression -----------------\\n\")\n",
    "print ('Runtime : ', end)\n",
    "# Evaluating the model on training data\n",
    "labelsAndPreds = testData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "acc = labelsAndPreds.filter(lambda lp: lp[0] == lp[1]).count() / float(testData.count())\n",
    "print(\"Test Accurancy = \" + str(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
